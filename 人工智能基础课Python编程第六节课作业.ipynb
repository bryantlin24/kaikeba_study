{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.kaikeba.com/web/kkb_index/img_index_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础课第一部分（python）第六次作业  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，大家好!  欢迎各位开始学习我们的人工智能课程。  \n",
    "这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。  \n",
    "这门课程结束后，希望大家掌握Python语言以及人工智能基础知识，对CV，NLP，RS领域有一定深入的理解与编程能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 如何提交作业\n",
    "有关代码的作业，在上课平台上提交，具体方式，见作业提交指南\n",
    "## 2. 作业截止时间\n",
    "作业能帮助你回顾课堂内容，你又可以通过作业进行代码实操。咱们可要认真、及时的完成作业哦！自布置作业起两周内提交，助教及时批改作业哦～逾期提交不批改。（特殊情况，请找班主任请假。）\n",
    "## 3. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**理论部分**  \n",
    "无"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**编程实践部分**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 爬虫数据集筛选及保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('杭州', '2019-01-01', '良', '73', '205', '53', '72', '8', '39', '0.90', '20')\n",
      "('杭州', '2019-01-02', '良', '90', '202', '66', '90', '9', '48', '0.95', '21')\n",
      "('杭州', '2019-01-04', '良', '79', '205', '58', '83', '7', '53', '1.39', '5')\n",
      "('杭州', '2019-01-05', '优', '31', '43', '21', '27', '6', '42', '1.18', '6')\n",
      "('杭州', '2019-01-06', '良', '55', '131', '38', '51', '7', '42', '1.67', '10')\n",
      "('杭州', '2019-01-07', '良', '55', '106', '38', '58', '8', '55', '1.25', '4')\n",
      "('杭州', '2019-01-08', '良', '64', '161', '45', '62', '8', '47', '1.04', '18')\n",
      "('杭州', '2019-01-09', '良', '66', '209', '47', '66', '9', '51', '1.06', '17')\n",
      "('杭州', '2019-01-10', '优', '36', '82', '22', '31', '7', '49', '0.82', '6')\n",
      "('杭州', '2019-01-11', '优', '26', '30', '15', '21', '7', '48', '0.92', '3')\n",
      "('杭州', '2019-01-12', '优', '39', '53', '26', '37', '7', '37', '1.12', '13')\n",
      "('杭州', '2019-01-13', '良', '84', '150', '62', '86', '9', '42', '1.17', '22')\n",
      "('杭州', '2019-01-16', '良', '84', '288', '60', '90', '9', '37', '0.99', '26')\n",
      "('杭州', '2019-01-17', '良', '81', '187', '59', '97', '9', '46', '0.99', '23')\n",
      "('杭州', '2019-01-18', '良', '91', '203', '66', '117', '8', '53', '1.12', '27')\n",
      "('杭州', '2019-01-19', '良', '92', '199', '68', '119', '7', '55', '1.26', '10')\n",
      "('杭州', '2019-01-21', '良', '73', '192', '52', '84', '9', '47', '0.83', '26')\n",
      "('杭州', '2019-01-22', '良', '92', '217', '68', '110', '8', '49', '1.00', '36')\n",
      "('杭州', '2019-01-23', '良', '95', '217', '70', '116', '8', '58', '1.10', '36')\n",
      "('杭州', '2019-01-26', '优', '49', '78', '28', '50', '7', '24', '0.66', '52')\n",
      "('杭州', '2019-01-27', '良', '60', '100', '38', '68', '6', '41', '0.74', '35')\n",
      "('杭州', '2019-01-28', '良', '92', '225', '67', '107', '10', '59', '0.98', '15')\n",
      "('杭州', '2019-01-31', '优', '39', '62', '22', '37', '5', '23', '0.71', '45')\n",
      "data saved in  ./data.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 以前遇到过的函数\n",
    "\n",
    "def build_url(city_coding, year=None, month=None):\n",
    "    \"\"\"\n",
    "    创建网页链接\n",
    "    paramters:\n",
    "        city_coding: 城市名称(英文)\n",
    "        year: 年份\n",
    "        month: 月份\n",
    "    return:\n",
    "        url: 可访问的链接\n",
    "    \"\"\"\n",
    "    BASE = 'http://www.tianqihoubao.com/aqi/'\n",
    "    city_base_url = BASE + '{}.html'\n",
    "    city_date_base_url = BASE + '{}-{}{}.html'\n",
    "    \n",
    "    if year is not None and month is not None:\n",
    "        month = str(month) if month >= 10 else '0' + str(month)\n",
    "        return city_date_base_url.format(city_coding, year, month)\n",
    "    else:\n",
    "        return city_base_url.format(city_coding)\n",
    "\n",
    "\n",
    "def parse(url, city_name):\n",
    "    \"\"\"\n",
    "    抓取网页信息\n",
    "    parameters:\n",
    "        url: 需要抓取的网页链接\n",
    "        city_name: 城市名称(用于数据标识)\n",
    "    returns:\n",
    "        result: 抓取的信息\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "        \n",
    "        soup = BeautifulSoup(html)\n",
    "        data_table = soup.table\n",
    "        \n",
    "        content = data_table.contents\n",
    "        \n",
    "        result = []\n",
    "        for index, c in enumerate(content[1::2]):\n",
    "                if index == 0:\n",
    "                    result.append(tuple(['城市'] + c.text.split()))\n",
    "                else:\n",
    "                    result.append(tuple([city_name] + c.text.split()))\n",
    "        return result\n",
    "    \n",
    "    else:\n",
    "        if response.status_code == 403:\n",
    "            print('403 Forbidden! 抓取太快你被拉黑啦~')\n",
    "\n",
    "            \n",
    "def save(data, file):\n",
    "    # 完成数据保存到文件\n",
    "    # your code here\n",
    "    with open(file, 'w',encoding='utf-8') as f:\n",
    "        f.write(data)\n",
    "    # 提示：用什么方法将数据写入文件？\n",
    "    \n",
    "    print('data saved in ', file)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datas = []\n",
    "    for i in range(1, 2):\n",
    "        url = build_url('hangzhou', 2019, i)\n",
    "        data = parse(url, '杭州')\n",
    "        datas.extend(data)\n",
    "    # print(datas)\n",
    "    \n",
    "    # 只保留质量等级优 良 数据\n",
    "    # your code here\n",
    "    datas='\\n'.join(list(map(str,filter(lambda x:x[2]=='优' or x[2]=='良', datas))))\n",
    "    # 提示：用什么方法对数据进行筛选？\n",
    "    print(datas)\n",
    "    # 保存数据\n",
    "    save(datas, './data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 西瓜数据集保存及读取  \n",
    "添加编号列，并将数据集写入到`machine_learning.csv`文件，使用pandas读取验证文件是否有效(无错即可)。  \n",
    "添加一条记录，`青绿 硬挺 浊响 稍糊 平坦 硬滑 0.666 0.111 好`  \n",
    "再使用普通文件读取将数据集读取出来，列名读取到`columns`，数据(带编号)读取到`datalist`  \n",
    "在所有数据中过滤出色泽='浅白'的数据  \n",
    "在所有数据中过滤出密度大于0.5的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   编号  色泽  根蒂  敲声  纹理  脐部  触感     密度    含糖率 好瓜\n",
      "0   1  青绿  蜷缩  浊响  清晰  凹陷  硬滑  0.697  0.460  是\n",
      "1   2  乌黑  蜷缩  沉闷  清晰  凹陷  硬滑  0.774  0.376  是\n",
      "2   3  乌黑  蜷缩  浊响  清晰  凹陷  硬滑  0.634  0.264  是\n",
      "3   4  青绿  蜷缩  沉闷  清晰  凹陷  硬滑  0.608  0.318  是\n",
      "4   5  浅白  蜷缩  浊响  清晰  凹陷  硬滑  0.556  0.215  是\n",
      "True\n",
      "True\n",
      "[['5', '浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '0.556', '0.215', '是'], ['11', '浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑', '0.245', '0.057', '否'], ['12', '浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘', '0.343', '0.099', '否'], ['14', '浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '0.657', '0.198', '否'], ['16', '浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '0.593', '0.042', '否']]\n",
      "[['1', '青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '0.697', '0.460', '是'], ['2', '乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '0.774', '0.376', '是'], ['3', '乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '0.634', '0.264', '是'], ['4', '青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑', '0.608', '0.318', '是'], ['5', '浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑', '0.556', '0.215', '是'], ['9', '乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑', '0.666', '0.091', '否'], ['13', '青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑', '0.639', '0.161', '否'], ['14', '浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑', '0.657', '0.198', '否'], ['16', '浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑', '0.593', '0.042', '否'], ['17', '青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑', '0.719', '0.103', '否'], ['18', '青绿', '硬挺', '浊响', '稍糊', '平坦', '硬滑', '0.666', '0.111', '是']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = \\\n",
    "\"\"\"色泽 根蒂 敲声 纹理 脐部 触感 密度 含糖率 好瓜\n",
    "青绿 蜷缩 浊响 清晰 凹陷 硬滑 0.697 0.460 是\n",
    "乌黑 蜷缩 沉闷 清晰 凹陷 硬滑 0.774 0.376 是\n",
    "乌黑 蜷缩 浊响 清晰 凹陷 硬滑 0.634 0.264 是\n",
    "青绿 蜷缩 沉闷 清晰 凹陷 硬滑 0.608 0.318 是\n",
    "浅白 蜷缩 浊响 清晰 凹陷 硬滑 0.556 0.215 是\n",
    "青绿 稍蜷 浊响 清晰 稍凹 软粘 0.403 0.237 是\n",
    "乌黑 稍蜷 浊响 稍糊 稍凹 软粘 0.481 0.149 是\n",
    "乌黑 稍蜷 浊响 清晰 稍凹 硬滑 0.437 0.211 是\n",
    "乌黑 稍蜷 沉闷 稍糊 稍凹 硬滑 0.666 0.091 否\n",
    "青绿 硬挺 清脆 清晰 平坦 软粘 0.243 0.267 否\n",
    "浅白 硬挺 清脆 模糊 平坦 硬滑 0.245 0.057 否\n",
    "浅白 蜷缩 浊响 模糊 平坦 软粘 0.343 0.099 否\n",
    "青绿 稍蜷 浊响 稍糊 凹陷 硬滑 0.639 0.161 否\n",
    "浅白 稍蜷 沉闷 稍糊 凹陷 硬滑 0.657 0.198 否\n",
    "乌黑 稍蜷 浊响 清晰 稍凹 软粘 0.360 0.370 否\n",
    "浅白 蜷缩 浊响 模糊 平坦 硬滑 0.593 0.042 否\n",
    "青绿 蜷缩 沉闷 稍糊 稍凹 硬滑 0.719 0.103 否\"\"\"\n",
    "\n",
    "# 将数据写入csv文件\n",
    "# your code here \n",
    "file = r'machine_learning.csv' # 文件名称，学员可修改或不修改\n",
    "with open(file, 'w',encoding='utf-8') as f:\n",
    "        #f.write(data)\n",
    "        dataset=dataset.replace(' ',',')\n",
    "        dataset = dataset.split('\\n')\n",
    "        f.write('编号,'+dataset[0]+'\\n')\n",
    "        for i in range(1,len(dataset)):\n",
    "            f.write(str(i)+','+dataset[i]+'\\n')\n",
    "            \n",
    "inser_data = '18 青绿 硬挺 浊响 稍糊 平坦 硬滑 0.666 0.111 是'\n",
    "with open(file,'a',encoding='utf-8') as f:\n",
    "    f.write(inser_data.replace(' ',','))\n",
    "        \n",
    "        \n",
    "\n",
    "# 向csv文件中加入一条新的数据（数据已给出）\n",
    "# your code here\n",
    "# 注意每一行数据的间隔符号是什么  \\n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 查看全体数据\n",
    "df = pd.read_csv(file)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# 读取文件存储的数据\n",
    "# your code here\n",
    "# columns是指列标签\n",
    "# datalist指全体数据内容，每一行数据应为一个列表\n",
    "columns = []\n",
    "datalist = []\n",
    "with open(file,encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        data = line.strip().split(',')\n",
    "        datalist.append(data)\n",
    "columns = datalist.pop(0)\n",
    "\n",
    "# 验证数据信息是否相符\n",
    "print(columns==['编号', '色泽', '根蒂', '敲声', '纹理', '脐部', '触感', '密度', '含糖率', '好瓜'])\n",
    "print(datalist[-1]==['18', '青绿', '硬挺', '浊响', '稍糊', '平坦', '硬滑', '0.666', '0.111', '是'])\n",
    "\n",
    "print(list(filter(lambda x:x[1]=='浅白', datalist)))\n",
    "print(list(filter(lambda x: float(x[7])>0.5, datalist)))\n",
    "# 在所有数据中过滤出色泽='浅白'的数据\n",
    "# 在所有数据中过滤出密度大于0.5的数据\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 评阅点：\n",
    "1. 使用,替代空格，并添加编号写入csv文件，可通过pandas进行验证  \n",
    "2. 使用a模式进行数据添加\n",
    "3. 读取数据文件并存到对应变量，注意strip，可根据最后验证代码进行验证  \n",
    "4. 使用filter lambda配合进行条件过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
